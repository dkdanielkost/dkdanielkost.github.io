<!DOCTYPE html>
<html>
<body style="font-family:verdana;">

<h1>Repository of Daniel Kost-Stephenson's data science work</h1>
<h4>This repository is composed of individual as well as coursework from the MSc in Data Science at Columbia University</h4>
<hr>
<h2>Individual work</h2>
<hr>
<p>Visualization of LDA topic modelling of tweets directed at Hillary Clinton and Donald Trump during the second presidential debate.</br>
<A HREF="Topic_modelling.html">Topic modelling visualization</A></p>

<p>Baseball visuals of Lahman database examining the evolution of various batting statistics over time. Specifically, I examined how power numbers changed during the "steroid era" of baseball.</br>
<A HREF="Baseball_visuals.html">Freelance baseball visuals</A></p>

<p>Visuals of PitchFx sample data. This was part of a recruitment process, so the identity of the pitcher is unknown, but the data contains several starts as well as relief appearances by the same pitcher.</br>
<A HREF="PitchFxAngels.html">PitchFx visuals</A></p>

<hr>
<h2>Columbia University course work</h2>
<hr>

<h3>Exploratory Data Analysis and Visualization</h3>

<h4>Assignment 2</h4>
<p>The second team assignment in the Exploratory Data Analysis and Visualization class focused on exploratory analysis of flood data taken over the course of the past 30 years. My contributions were some of the wordclouds as well as the marked map and interactive choropleth maps.</br>
<A HREF="EDAV2/Project2_No_Sunshine.html">EDAV flood data analysis</A></p>

<h4>Assignment 3</h4>
<p>The third assignment for EDAV involved analyzing healthcare information by state. I contributed the choropleth animated map and bubble chart gif near the very bottom of the file.</br>
<A HREF="EDAV3/SixPack.html">EDAV healthcare analysis</A></p>

<h4>Final Project</h4>
<p>The final project in my exploratory data analysis and visualization class involved exploratory analysis of 311 complains around the NYC area. My contributions included the animated density gif (with colored histogram) and the nine complaint reconstructions (for noise, taxis and plumbing) by zip code. I also dedicated a significant amount of time to the modelling portion of the project.</br>
<A HREF="EDAV_finalproject/exploratoryData.html">EDAV final project</A></p>
<hr>
<h3>Machine Learning</h3>

<h4>Assignment 1</h4>
<p>The first machine learning assignment consisted of two main problems: classification of handwritten digits and classification of messages belonging to 20 different news sources based based on the frequency of words contained in each document. More specifically, we were tasked with implementing a vectorized version of K-Nearest Neighbors for the digit classification and a version of a Naive Bayes generative model for the message classification. A test accuracy of 94.38% was achieved for the first problem and an accuracy of 76.95% was obtained for the second.</br>
<A HREF="dpk2124_HW1/ML_assignment1_Final.py">Python code</A></br>
<A HREF="dpk2124_HW1/Assignment 1 write up.pdf">Write-up</A></p>

<h4>Assignment 2</h4>
<p>The second machine learning assignment involved classifying emails as spam or not spam. Several methods were implemented, namely averaged perceptron, logistic regression, linear discriminant analysis (LDA), quadratic discriminant analysis (QDA), and logistic regression and averaged perceptron with an extended feature map (kernelized). One important note is that the averaged perceptron classifier and a cross validation function were implemented without using any library functions (no use of sklearn or other packages with easily implemented perceptron and CV functions). The full results are presented in the report, but the method with the best score was logistic regression with an extended feature map at 92.46%.</br>
<A HREF="dpk2124_HW2/ML_assignment2_Final.py">Python code</A></br>
<A HREF="dpk2124_HW2/Assignment 2 write up.pdf">Write-up</A></p>

<h4>Assignment 3</h4>
The third assignment consised of two theoretical questions and one short programming question. The programming question simply involved using linear regression to predict the median value of occupied homes in the Boston housing dataset. In addition, we experimented with regularization techniques such as the Lasso and Ridge Regression. The report can be found below.</br>
<A HREF="dpk2124_HW3/dpk2124_HW3.pdf">Write-up</A></p>

<h4>Assignment 4</h4>
The last assignment involed implementing an alternating leaast-squares (ALS) algorithm to assign ratings to each user/movie pair on a sample of Netflix data. The data was given as a sparse matrix, with only some user/movies having values. Our task was to predict ratings for the user/movie pairs without values. ALS was implemented without the use of any library functions, by updating one parameter while holding the others constant. The log likelihood function was the loss function and the test mean squared error was found to be approximately 0.93.</br>
<A HREF="dpk2124_HW4/HW4_Final.py">Python code</A></br>
<A HREF="dpk2124_HW2/HW4_Q1.pdf">Programming write-up</A></br>
<A HREF="dpk2124_HW2/HW4_Q2.pdf">Theoretical write-up</A></p>

<h4>Final Project</h4>
<p>Our final machine learning project consisted of a binary classification problem with numeric and categorical data. It consisted of dialogue (observations) between two people and the label indicated if that dialogue led to successful cooperation. The project was in the format of a Kaggle competition, where each of the teams were ranked based on the test scores they obtained after submitting.Our methodology and results are outlined in the full report.</br>
<A HREF="ML Project/Final_Project.pdf">Python code</A></p>
<hr>
</body>
</html>